# 完整训练与环境交互流程

## 1. 训练入口 (train.py)

```python
# 文件: simulation/videomimic_gym/legged_gym/scripts/train.py
if __name__ == '__main__':
    args, unknown = get_args()
    train(args, unknown)

def train(args, unknown):
    # 1.1 创建环境
    env, env_cfg = task_registry.make_env(name=args.task, args=args)
    # env = G1DeepMimic(cfg, sim_params, physics_engine, sim_device, headless)
    
    # 1.2 创建PPO训练器
    ppo_runner, train_cfg = task_registry.make_alg_runner(env=env, name=args.task)
    # ppo_runner = OnPolicyRunner(env, train_cfg)
    
    # 1.3 开始学习
    ppo_runner.learn(num_learning_iterations=train_cfg.runner.max_iterations)
```

## 2. 训练主循环 (OnPolicyRunner.learn)

```python
# 文件: simulation/videomimic_rl/rsl_rl/runners/on_policy_runner.py
def learn(self, num_learning_iterations):
    obs = self.env.get_observations()  # 初始观测
    
    for it in range(num_iterations):  # 每次迭代
        # ========== 2.1 数据收集阶段 ==========
        for i in range(self.num_steps_per_env):  # 例如：24步
            # 2.1.1 策略网络推理得到动作
            actions = self.alg.act(obs)
            # actions = policy_network(obs)
            
            # 2.1.2 环境步进（关键！）
            obs, rewards, dones, infos = self.env.step(actions)
            
            # 2.1.3 存储经验
            self.alg.process_env_step(rewards, dones, infos)
        
        # ========== 2.2 策略更新阶段 ==========
        self.alg.compute_returns(obs)  # 计算GAE优势和回报
        self.alg.update()              # PPO更新
```

## 3. 环境步进详细流程 (LeggedRobot.step)

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/legged_robot.py
def step(self, actions):
    # ========== 3.1 动作处理 ==========
    # 3.1.1 添加噪声
    actions = actions + self.actions_offset_seed * noise_scale
    
    # 3.1.2 裁剪动作
    actions_clipped = torch.clip(actions, -clip_actions, clip_actions)
    
    # 3.1.3 动作平滑（一阶低通滤波）
    self.actions = beta * actions_clipped + (1-beta) * self.actions
    
    # ========== 3.2 物理仿真 ==========
    self.render()  # 渲染（如果有viewer）
    
    for _ in range(self.cfg.control.decimation):  # 例如：4次
        # 3.2.1 计算扭矩
        self.torques = self._compute_torques(self.actions)
        # torques = (actions_scaled + target_motors - dof_pos) * p_gains - d_gains * dof_vel
        
        # 3.2.2 应用扭矩到仿真器
        self.gym.set_dof_actuation_force_tensor(self.sim, self.torques)
        
        # 3.2.3 物理仿真步进
        self.gym.simulate(self.sim)
        
        # 3.2.4 获取仿真结果
        self.gym.fetch_results(self.sim, True)
        self.gym.refresh_dof_state_tensor(self.sim)
    
    # ========== 3.3 后处理 ==========
    self.post_physics_step()  # 详见第4节
    
    # ========== 3.4 返回观测和奖励 ==========
    return self.obs_dict, self.rew_buf, self.reset_buf, self.extras
```

## 4. 物理步进后处理 (LeggedRobot.post_physics_step)

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/legged_robot.py
def post_physics_step(self):
    # ========== 4.1 刷新仿真状态 ==========
    self.gym.refresh_actor_root_state_tensor(self.sim)     # 根部状态
    self.gym.refresh_net_contact_force_tensor(self.sim)    # 接触力
    self.gym.refresh_rigid_body_state_tensor(self.sim)     # 刚体状态
    
    self.episode_length_buf += 1  # 步数计数
    
    # ========== 4.2 更新基础量 ==========
    self.base_pos = self.root_states[:, 0:3]
    self.base_quat = self.root_states[:, 3:7]
    self.base_lin_vel = quat_rotate_inverse(self.base_quat, self.root_states[:, 7:10])
    self.base_ang_vel = quat_rotate_inverse(self.base_quat, self.root_states[:, 10:13])
    self.projected_gravity = quat_rotate_inverse(self.base_quat, self.gravity_vec)
    
    # ========== 4.3 调用子类的后处理 ==========
    self._post_physics_step_callback()  # RobotDeepMimic重写此函数
    
    # ========== 4.4 更新传感器 ==========
    for sensor_name, sensor in self.sensors.items():
        sensor.update_buffers(episode_step=self.episode_length_buf)
    
    # ========== 4.5 核心三步 ==========
    # 4.5.1 检查终止条件
    self.check_termination()
    # self.reset_buf = 判断是否需要重置
    # self.time_out_buf = 判断是否超时
    
    # 4.5.2 计算奖励
    self.compute_reward()
    # self.rew_buf = sum(scale_i * reward_i)
    
    # 4.5.3 重置环境（如果需要）
    env_ids = self.reset_buf.nonzero(as_tuple=False).flatten()
    if len(env_ids) > 0:
        self.reset_idx(env_ids)
    
    # ========== 4.6 计算观测 ==========
    self.compute_observations()
    
    # ========== 4.7 更新历史 ==========
    self.last_last_actions = self.last_actions
    self.last_actions = self.actions
    self.last_dof_vel = self.dof_vel
```

## 5. DeepMimic特定的后处理

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/robot_deepmimic.py
def _post_physics_step_callback(self):
    # ========== 5.1 更新回放数据索引 ==========
    self.replay_data_loader.increment_indices()
    
    # ========== 5.2 更新目标参考数据 ==========
    self.update_replay_data()
    # state = replay_data_loader.get_current_data()
    # self.target_root_pos = state.root_pos
    # self.target_root_quat = state.root_quat
    # self.target_motors = state.motors
    # self.target_link_pos = state.link_pos
    # self.target_contacts = state.contacts
    
    # ========== 5.3 计算跟踪误差 ==========
    self.link_pos_error = self._compute_link_pos_error()
    # = self.env_rigid_body_pos[:, tracked_indices] - self.target_link_pos
    
    self.link_vel_error = self._compute_link_vel_error()
    # = self.rigid_body_vel[:, tracked_indices] - self.target_link_vel
    
    # ========== 5.4 可视化（可选）==========
    if self.cfg.deepmimic.viz_replay:
        self.viz_replay_data(set_robot_pos=False)
```

## 6. 计算观测详解

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/legged_robot.py
def compute_observations(self):
    # ========== 6.1 G1DeepMimic特定：记录片段分布 ==========
    # 统计当前所有环境使用的动作片段分布
    self._log_clip_distribution()
    
    # ========== 6.2 调用各个观测函数 ==========
    self.obs_dict = {}
    
    # 6.2.1 本体感知
    self.obs_dict['torso_real'] = self._obs_torso_real()
    # [ang_vel, gravity, dof_pos, dof_vel, actions]
    
    # 6.2.2 参考动作信息（核心！）
    self.obs_dict['deepmimic'] = self._obs_deepmimic()
    # [link_heights, root_quat, root_pos, joints, link_pos, link_vel, contacts]
    
    # 6.2.3 目标速度
    self.obs_dict['deepmimic_lin_ang_vel'] = self._obs_deepmimic_lin_ang_vel()
    # [linear_velocity, angular_velocity]
    
    # 6.2.4 相对位置
    self.obs_dict['torso_xy_rel'] = self._obs_torso_xy_rel()
    # target_torso_pos - current_torso_pos (in local frame)
    
    self.obs_dict['torso_yaw_rel'] = self._obs_torso_yaw_rel()
    # target_heading - current_heading
    
    # 6.2.5 目标关节
    self.obs_dict['target_joints'] = self._obs_target_joints()
    # (target_motors - default_dof_pos) * scale
    
    # 6.2.6 目标姿态
    self.obs_dict['target_root_roll'] = self._obs_target_root_roll()
    self.obs_dict['target_root_pitch'] = self._obs_target_root_pitch()
    
    # 6.2.7 地形信息
    self.obs_dict['terrain_height'] = self.get_sensor_data('terrain_height')
    
    # 6.2.8 教师索引（多教师）
    self.obs_dict['teacher_checkpoint_index'] = self._obs_teacher_checkpoint_index()
    
    # ========== 6.3 处理观测历史 ==========
    if self.history_handler is None:
        self._init_history_handler()
    
    # 更新历史缓冲区
    for obs_name in self.cfg.env.obs_history.keys():
        self.history_handler.update(obs_name, self.obs_dict[obs_name])
    
    # 添加历史观测到obs_dict
    for obs_name, history_length in self.cfg.env.obs_history.items():
        self.obs_dict[f'history_{obs_name}'] = self.history_handler.get_obs(obs_name)
    
    # ========== 6.4 G1DeepMimic特定：定期更新权重 ==========
    if it % log_interval == 0:
        # 计算成功率
        success_rates = self._compute_and_log_success_rates()
        
        # 更新自适应权重
        if weighting_strategy == 'success_rate_adaptive':
            self.replay_data_loader.update_adaptive_weights(success_rates)
```

## 7. 计算奖励详解

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/legged_robot.py
def compute_reward(self):
    # ========== 7.1 计算所有奖励项 ==========
    self.rew_buf[:] = 0.
    
    for name in self.reward_functions:
        # 7.1.1 调用奖励函数
        rew = self.reward_functions[name]() * self.reward_scales[name]
        # 例如：_reward_joint_pos_tracking() * 120.0
        
        # 7.1.2 累加奖励
        self.rew_buf += rew
        
        # 7.1.3 记录到extras（用于日志）
        self.episode_sums[name] += rew
    
    # ========== 7.2 处理终止情况 ==========
    if self.cfg.rewards.only_positive_rewards:
        self.rew_buf[:] = torch.clip(self.rew_buf[:], min=0.)
    
    # 重置时的特殊处理
    self.episode_sums["termination"] += self.reset_buf * self.reward_scales["termination"]
```

## 8. 关键奖励函数示例

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/robot_deepmimic.py

# 8.1 关节位置跟踪
@torch.jit.export
def _reward_joint_pos_tracking(self):
    motor_pos_error = self.dof_pos - self.target_motors
    k = self.cfg.rewards.joint_pos_tracking_k  # 例如：2.0
    return torch.exp(-torch.pow(motor_pos_error, 2).sum(dim=-1) * k)
    # 奖励 = exp(-误差^2 * k)
    # 误差越小，奖励越接近1

# 8.2 身体部位位置跟踪
def _reward_link_pos_tracking(self):
    k = self.cfg.rewards.link_pos_tracking_k  # 例如：5.0
    return torch.exp(-torch.pow(self.link_pos_error, 2).sum(dim=1).sum(dim=1) * k)

# 8.3 躯干位置跟踪
@torch.jit.export
def _reward_torso_pos_tracking(self):
    torso_pos_error = self.env_rigid_body_pos[:, self.torso_index] - \
                      self.target_extra_link_pos[:, self.extra_link_torso_index]
    k = self.cfg.rewards.torso_pos_tracking_k  # 例如：50.0
    return torch.exp(-torch.pow(torso_pos_error, 2).sum(dim=-1) * k)

# 8.4 接触匹配
def _reward_feet_contact_matching(self):
    contact = self.contact_forces[:, self.feet_indices, 2] > 1.
    desired_contact = self.target_contacts
    return torch.sum((contact == desired_contact).float(), dim=1)
    # 每只脚接触状态匹配得1分

# 8.5 动作平滑性（惩罚）
def _reward_action_rate(self):
    return torch.sum(torch.square(self.actions - self.last_actions), dim=1)
    # 动作变化越大，惩罚越大
```

## 9. 检查终止条件

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/robot_deepmimic.py
def check_termination(self):
    # ========== 9.1 接触终止 ==========
    self.reset_buf = torch.any(
        torch.norm(self.contact_forces[:, self.termination_contact_indices, :], dim=-1) > 1., 
        dim=1
    )
    
    # ========== 9.2 跟踪误差终止 ==========
    link_pos_error = torch.norm(self.link_pos_error, dim=-1)
    link_pos_error_threshold = self.cfg.deepmimic.link_pos_error_threshold  # 例如：0.5米
    self.reset_buf |= torch.any(link_pos_error > link_pos_error_threshold, dim=1) & \
                      (self.episode_length_buf >= 2)
    
    # ========== 9.3 大接触力终止 ==========
    if self.cfg.asset.terminate_after_large_feet_contact_forces:
        self.reset_buf |= torch.any(
            torch.norm(self.contact_forces[:, self.feet_indices, :], dim=-1) > 
            self.cfg.asset.large_feet_contact_force_threshold, 
            dim=1
        )
    
    # ========== 9.4 超时 ==========
    self.time_out_buf = self.episode_length_buf >= self.max_episode_length
    self.reset_buf |= self.time_out_buf
    
    # ========== 9.5 G1DeepMimic特定：记录成功率 ==========
    reset_env_ids = torch.where(self.reset_buf)[0]
    if len(reset_env_ids) > 0:
        # 超时视为成功
        is_success = self.time_out_buf[reset_env_ids].long()
        
        # 记录到成功率历史
        unique_clip_indices = self.original_idx_to_unique_idx[
            self.replay_data_loader.episode_indices[reset_env_ids]
        ]
        self.clip_success_history[unique_clip_indices, history_pointers] = is_success
```

## 10. 重置环境

```python
# 文件: simulation/videomimic_gym/legged_gym/envs/base/robot_deepmimic.py
def reset_idx(self, env_ids):
    # ========== 10.1 选择新的动作片段 ==========
    env_mask = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
    env_mask[env_ids] = True
    
    # 根据权重采样新片段
    reset_episode_length = self.replay_data_loader.reset(env_mask)
    self.max_episode_length = torch.min(
        truncate_length * torch.ones_like(reset_episode_length), 
        reset_episode_length
    )
    
    # ========== 10.2 获取初始状态 ==========
    self.reset_start_state = self.replay_data_loader.get_current_data()
    
    # ========== 10.3 设置地形偏移 ==========
    self.env_offsets[env_ids] = self.terrain.get_terrain_offset(
        self.reset_start_state.clip_index[env_ids]
    )
    
    # ========== 10.4 调用父类重置 ==========
    super().reset_idx(env_ids)
    # 重置DOF、根部状态、传感器等
    
    # ========== 10.5 匹配初始动作 ==========
    self.actions[env_ids] = (self.dof_pos[env_ids] - self.default_dof_pos) / \
                            self.cfg.control.action_scale
    self.last_actions[env_ids] = self.actions[env_ids]
```

## 完整流程总结
